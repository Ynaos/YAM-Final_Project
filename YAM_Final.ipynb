{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ynaos/YAM-Final_Project/blob/main/YAM_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53a25e8",
      "metadata": {
        "id": "b53a25e8"
      },
      "source": [
        "## Setup and Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- This Google Colab code installs several important Python libraries used for natural language processing, machine learning, and model optimization. The first line installs or upgrades Hugging Face tools such as transformers for working with pretrained NLP models, datasets for efficient data handling, and accelerate for speeding up model training across different hardware. It also adds Ray Tune and Optuna, two powerful frameworks for hyperparameter tuning. The second line installs TensorFlow for deep learning, openpyxl for working with Excel files, and scikit-learn for traditional machine learning methods, along with transformers again to ensure compatibility. Finally, the last line installs VADER Sentiment, a lightweight sentiment analysis tool commonly used for short texts like social media posts. Together, these libraries enable you to build, train, evaluate, and optimize NLP models within your Colab environment."
      ],
      "metadata": {
        "id": "DFUf3qobqTVk"
      },
      "id": "DFUf3qobqTVk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b94ed4a",
      "metadata": {
        "collapsed": true,
        "id": "5b94ed4a"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate ray[tune] optuna -U\n",
        "!pip install transformers tensorflow openpyxl scikit-learn -q\n",
        "!pip install vaderSentiment -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "- This section of the code imports all the necessary libraries and sets up the basic configuration for data processing, sentiment analysis, machine learning, and model training. It begins by importing standard Python modules like random, os, pandas, and numpy for data handling, randomness control, file access, and numerical operations. The train_test_split function from scikit-learn is used to divide the dataset into training and testing sets. It also imports VADER SentimentIntensityAnalyzer for rule-based sentiment scoring. The LabelEncoder is included to convert categorical text labels into numeric form.\n",
        "\n",
        "- From Hugging Face Transformers, it imports AutoTokenizer and AutoModelForSequenceClassification to load pretrained models for text classification, as well as TrainingArguments, Trainer, and pipeline for fine-tuning and running NLP models. The torch library provides PyTorch support for deep learning computations. The datasets module is used to structure data into a format suitable for model training. Finally, scikit-learn metrics like accuracy, precision, recall, F1-score, classification reports, and confusion matrices are imported to evaluate model performance."
      ],
      "metadata": {
        "id": "NAVMXgQxq87e"
      },
      "id": "NAVMXgQxq87e"
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Imports and basic config\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          TrainingArguments, Trainer, pipeline)\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "dOpTLUccyn8z"
      },
      "id": "dOpTLUccyn8z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1) Load CSV and prepare HF Datasets**"
      ],
      "metadata": {
        "id": "vzyqeNogyLsd"
      },
      "id": "vzyqeNogyLsd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "This part of the code handles uploading your dataset, preparing it for training, and converting it into a format compatible with Hugging Face models. First, it imports the Google Colab file uploader and waits for you to upload a CSV file — in this case, data_mmda_traffic_spatial.csv. Once uploaded, the file is read into a pandas DataFrame and its size is printed. The code assumes your text column is named \"Tweet\", then removes any rows where this text is missing. It also checks if the dataset already contains a sentiment label column (e.g., label, sentiment, etc.).\n",
        "\n",
        "If no label column exists, the code automatically generates sentiment labels using VADER, a rule-based sentiment analyzer. It assigns each tweet a compound sentiment score and converts it into a numeric label:\n",
        "\n",
        "- 0 = Negative\n",
        "\n",
        "- 1 = Neutral\n",
        "\n",
        "- 2 = Positive\n",
        "\n",
        "These become training labels called label_id. If a label column does exist, the code instead encodes it numerically using LabelEncoder.\n",
        "\n",
        "Next, the dataset is split into training (85%) and validation (15%) sets, with balanced sentiment distribution using stratification. Finally, both splits are converted into Hugging Face Dataset objects, renamed to use \"text\" and \"label\" columns, and stored in a DatasetDict. This prepares your uploaded dataset — data_mmda_traffic_spatial.csv — for model fine-tuning in the next steps.-"
      ],
      "metadata": {
        "id": "o2HbHzMWrw2i"
      },
      "id": "o2HbHzMWrw2i"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Load CSV and prepare HF Dataset\n",
        "from google.colab import files\n",
        "# --- UPLOAD CSV ---\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the first uploaded CSV file\n",
        "for file_name in uploaded.keys():\n",
        "    df = pd.read_csv(file_name)\n",
        "    print(f\"✅ Loaded: {file_name} (shape={df.shape})\")\n",
        "    break\n",
        "\n",
        "# --- DEFINE TEXT COLUMN & CANDIDATES ---\n",
        "TEXT_COL = \"Tweet\"   # adjust if your text column name is different\n",
        "LOC_COL_CANDIDATES = [\"Location\", \"location\", \"place\", \"place_name\", \"area\", \"location_name\"]\n",
        "\n",
        "# Basic cleaning: drop rows without text\n",
        "df = df[df[TEXT_COL].notna()].reset_index(drop=True)\n",
        "\n",
        "# --- Detect existing label column (if any) ---\n",
        "label_col = None\n",
        "for potential in [\"label\", \"Label\", \"sentiment\", \"Sentiment\", \"sent\"]:\n",
        "    if potential in df.columns:\n",
        "        label_col = potential\n",
        "        break\n",
        "\n",
        "# --- If no gold labels are present, create VADER pseudo-labels ---\n",
        "if label_col is None:\n",
        "    # Install / import and initialize VADER (nltk VADER)\n",
        "    import nltk\n",
        "    nltk.download(\"vader_lexicon\", quiet=True)\n",
        "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Map compound score to numerical labels: NEGATIVE=0, NEUTRAL=1, POSITIVE=2\n",
        "    def vader_label_from_text(text):\n",
        "        c = sia.polarity_scores(str(text))[\"compound\"]\n",
        "        if c >= 0.05:\n",
        "            return 2\n",
        "        elif c <= -0.05:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    # Create VADER columns and numeric labels\n",
        "    df[\"vader_compound\"] = df[TEXT_COL].astype(str).apply(lambda t: sia.polarity_scores(t)[\"compound\"])\n",
        "    df[\"vader_label_id\"] = df[TEXT_COL].astype(str).apply(vader_label_from_text)\n",
        "\n",
        "    # Use VADER labels as the training label (safe default)\n",
        "    df[\"label_id\"] = df[\"vader_label_id\"]\n",
        "    label_col = \"vader_label_id\"\n",
        "    print(\"No gold label found — using VADER pseudo-labels (label_id) distribution:\")\n",
        "    print(df[\"label_id\"].value_counts())\n",
        "else:\n",
        "    # If a label column exists, encode it (textual or numeric)\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    df[\"label_id\"] = le.fit_transform(df[label_col].astype(str))\n",
        "    print(f\"Using gold label column '{label_col}' -> encoded to 'label_id'. Distribution:\")\n",
        "    print(df[\"label_id\"].value_counts())\n",
        "\n",
        "# --- Train/validation split (stratify on label_id) ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.15, stratify=df[\"label_id\"], random_state=42)\n",
        "\n",
        "# --- Convert to HuggingFace Dataset objects expected downstream ---\n",
        "from datasets import Dataset, DatasetDict\n",
        "hf_train = Dataset.from_pandas(train_df[[TEXT_COL, \"label_id\"]].rename(columns={TEXT_COL: \"text\", \"label_id\":\"label\"}))\n",
        "hf_val   = Dataset.from_pandas(val_df[[TEXT_COL, \"label_id\"]].rename(columns={TEXT_COL: \"text\", \"label_id\":\"label\"}))\n",
        "\n",
        "dataset_dict = DatasetDict({\"train\": hf_train, \"validation\": hf_val})\n",
        "\n",
        "print(\"Prepared HuggingFace DatasetDict with train/validation splits.\")\n",
        "print(\"Train size:\", len(dataset_dict[\"train\"]), \"Validation size:\", len(dataset_dict[\"validation\"]))\n"
      ],
      "metadata": {
        "id": "HZ1SEdAcygRv"
      },
      "id": "HZ1SEdAcygRv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2) Tokenizer & model factory**"
      ],
      "metadata": {
        "id": "RjJXS_UEyPjG"
      },
      "id": "RjJXS_UEyPjG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "This section of the code initializes the tokenizer and prepares your text data so it can be fed into a transformer model. It starts by selecting a pretrained model — in this case, distilbert-base-uncased, a lightweight and efficient version of BERT. Using this model name, it loads the corresponding AutoTokenizer, which is responsible for converting raw text into token IDs that the model can understand.\n",
        "\n",
        "Next, it defines a function called tokenize_fn(), which takes a batch of text and tokenizes it. **The tokenizer applies:**\n",
        "\n",
        "- truncation — cutting text that’s too long\n",
        "\n",
        "- padding — adding extra tokens so all sequences have equal length\n",
        "\n",
        "- max_length=128 — sets the maximum token size per input\n",
        "\n",
        "The code then applies this tokenizer function to the entire Hugging Face dataset using .map(), which efficiently processes it in batches. After tokenization, the original \"text\" column is removed since the model only needs tokenized input. The resulting dataset is put into PyTorch format, preparing it for training.\n",
        "\n",
        "Finally, it calculates how many different sentiment classes (labels) are present by checking the unique values in the label_id column. This allows the model to dynamically adapt whether your dataset has 2, 3, or more sentiment types."
      ],
      "metadata": {
        "id": "ei0derofsj-B"
      },
      "id": "ei0derofsj-B"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Tokenizer and model factory function\n",
        "MODEL_NAME = \"distilbert-base-uncased\"  # change if you want another model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized = dataset_dict.map(tokenize_fn, batched=True)\n",
        "tokenized = tokenized.remove_columns([\"text\"])\n",
        "tokenized.set_format(\"torch\")\n",
        "num_labels = len(np.unique(df[\"label_id\"]))\n"
      ],
      "metadata": {
        "id": "tJZdan9ZyrpO"
      },
      "id": "tJZdan9ZyrpO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3) Metrics function**"
      ],
      "metadata": {
        "id": "ZZkzP7uyyTaJ"
      },
      "id": "ZZkzP7uyyTaJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "This part of the code defines a function that calculates key evaluation metrics during model training and validation. The function compute_metrics() is used by the Hugging Face Trainer to measure how well the model is performing on the validation set.\n",
        "\n",
        "**It receives two inputs:**\n",
        "\n",
        "- logits — the raw predictions generated by the model\n",
        "\n",
        "- labels — the true sentiment labels from your dataset\n",
        "\n",
        "First, it converts the model’s logits into predicted class labels by taking the index of the highest score (argmax). Then, it computes four important metrics using scikit-learn:\n",
        "\n",
        "- Accuracy — the percentage of correct predictions\n",
        "\n",
        "- Precision (weighted) — how many predicted labels are correct, considering class imbalance\n",
        "\n",
        "- Recall (weighted) — how many of the true labels were correctly found\n",
        "\n",
        "- F1-score (weighted) — the harmonic balance between precision and recall\n",
        "\n",
        "Weighted averaging ensures that all sentiment classes in your dataset — especially from data_mmda_traffic_spatial.csv — are fairly represented, even if some appear less frequently. The function returns these metrics in a dictionary so the Trainer can log them during training."
      ],
      "metadata": {
        "id": "A1ZoJnsAs94z"
      },
      "id": "A1ZoJnsAs94z"
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec = precision_score(labels, preds, average=\"weighted\", zero_division=0)\n",
        "    rec = recall_score(labels, preds, average=\"weighted\", zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
        "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n"
      ],
      "metadata": {
        "id": "9Kkz-TvwyuIW"
      },
      "id": "9Kkz-TvwyuIW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4) Single-trial train function (returns validation metric)**"
      ],
      "metadata": {
        "id": "WGbPGhQe4hAt"
      },
      "id": "WGbPGhQe4hAt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "This part of the code defines a function that trains a sentiment classification model using a single set of hyperparameters and returns both the results and the saved model path. The function run_training_trial() accepts a dictionary of hyperparameters — including learning rate, batch size, number of epochs, and weight decay — and uses them to customize the training run.\n",
        "First, it loads a pretrained transformer model (based on distilbert-base-uncased) and adapts it for sentiment classification by setting num_labels according to the unique sentiment classes in your dataset data_mmda_traffic_spatial.csv. It then creates an output directory for the trial so that logs and checkpoints are stored separately.\n",
        "Next, **it sets up Hugging Face TrainingArguments, including:**\n",
        "\n",
        "\n",
        "- Training duration (num_train_epochs)\n",
        "\n",
        "\n",
        "- Batch size per device\n",
        "\n",
        "\n",
        "- Learning rate and weight decay\n",
        "\n",
        "\n",
        "- When to evaluate and save models (every epoch)\n",
        "\n",
        "\n",
        "- Automatic loading of the best-performing model\n",
        "\n",
        "\n",
        "- Accuracy as the metric to optimize\n",
        "\n",
        "\n",
        "- Mixed precision training (fp16) if GPU is available\n",
        "\n",
        "\n",
        "- The Trainer object is then created, combining:\n",
        "\n",
        "\n",
        "**The model**\n",
        "\n",
        "\n",
        "- Training/evaluation settings\n",
        "\n",
        "\n",
        "- Tokenized training and validation datasets\n",
        "\n",
        "\n",
        "- The previously defined compute_metrics function\n",
        "\n",
        "\n",
        "- The tokenizer for proper text processing\n",
        "\n",
        "\n",
        "The .train() command starts the actual fine-tuning process, training the model on your labeled tweets. When training is done, .evaluate() computes validation metrics. The best version of the model is saved into a best_model folder, and finally, the function returns both the evaluation results and the path where the fine-tuned model was stored."
      ],
      "metadata": {
        "id": "K8wDRhITtSSp"
      },
      "id": "K8wDRhITtSSp"
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Single training-run function (returns validation accuracy and path to saved model)\n",
        "def run_training_trial(hparams, trial_name=\"trial\"):\n",
        "    # hparams: dict with keys: learning_rate, per_device_train_batch_size, num_train_epochs, weight_decay\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
        "\n",
        "    out_dir = os.path.join(\"./results\", trial_name)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        num_train_epochs=hparams[\"num_train_epochs\"],\n",
        "        per_device_train_batch_size=hparams[\"per_device_train_batch_size\"],\n",
        "        per_device_eval_batch_size=max(8, hparams[\"per_device_train_batch_size\"]),\n",
        "        learning_rate=hparams[\"learning_rate\"],\n",
        "        weight_decay=hparams[\"weight_decay\"],\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        logging_dir=os.path.join(out_dir, \"logs\"),\n",
        "        logging_steps=50,\n",
        "        save_total_limit=1,\n",
        "        seed=42,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=[],\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized[\"train\"],\n",
        "        eval_dataset=tokenized[\"validation\"],\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Train (this will run for the specified num_train_epochs)\n",
        "    trainer.train()\n",
        "    eval_res = trainer.evaluate()\n",
        "    # Save\n",
        "    best_model_dir = os.path.join(out_dir, \"best_model\")\n",
        "    trainer.save_model(best_model_dir)\n",
        "    return eval_res, best_model_dir"
      ],
      "metadata": {
        "id": "WhL8Ttnm1O_G"
      },
      "id": "WhL8Ttnm1O_G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5) Random Search loop (simple)**"
      ],
      "metadata": {
        "id": "rW3NGD8GyZPe"
      },
      "id": "rW3NGD8GyZPe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "This part of the code performs random hyperparameter search to identify the best training configuration for your sentiment model based on accuracy. The function random_search() runs multiple training trials (default: 5) where each trial randomly selects a combination of hyperparameters from a predefined search space.\n",
        "\n",
        "**The search space includes:**\n",
        "\n",
        "- learning_rate: how fast the model updates (5e-6 to 5e-5)\n",
        "\n",
        "- batch size: number of samples processed per step (8, 16, or 32)\n",
        "\n",
        "- num_train_epochs: how many times the model sees the full dataset (1–3 for short mode)\n",
        "\n",
        "- weight_decay: regularization strength to prevent overfitting (0.0 to 0.1)\n",
        "\n",
        "**For every trial:**\n",
        "\n",
        "- A random set of values is selected from these ranges.\n",
        "\n",
        "- A trial name like \"rs_trial_0\" or \"rs_trial_1\" is assigned.\n",
        "\n",
        "- The run_training_trial() function is called to train the model using those settings on your dataset data_mmda_traffic_spatial.csv.\n",
        "\n",
        "- The resulting validation accuracy is retrieved.\n",
        "\n",
        "If a given trial achieves a higher accuracy than previous ones, its hyperparameters and model path are stored as the current best result.\n",
        "\n",
        "**After all trials finish, the code prints:**\n",
        "\n",
        "- The best accuracy achieved\n",
        "\n",
        "- The optimal hyperparameters that produced it\n",
        "\n",
        "**Finally, the random search is executed by calling:**\n",
        "\n",
        "N_TRIALS = 5\n",
        "best_hparams, best_model_dir, best_metric = random_search(n_trials=N_TRIALS, short_mode=True)\n",
        "\n",
        "\n",
        "**This runs 5 randomized training experiments and returns:**\n",
        "\n",
        "best_hparams — the most effective parameter set\n",
        "\n",
        "best_model_dir — where the best model is saved\n",
        "\n",
        "best_metric — the highest validation accuracy achieved"
      ],
      "metadata": {
        "id": "nWTorRA5t-eZ"
      },
      "id": "nWTorRA5t-eZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Random search over hyperparameters\n",
        "def random_search(n_trials=5, short_mode=True):\n",
        "    best_metric = -999\n",
        "    best_hparams = None\n",
        "    best_model_dir = None\n",
        "\n",
        "    # simple search space\n",
        "    search_space = {\n",
        "        \"learning_rate\": [5e-6, 1e-5, 2e-5, 3e-5, 5e-5],\n",
        "        \"per_device_train_batch_size\": [8, 16, 32],\n",
        "        \"num_train_epochs\": [1, 2, 3] if short_mode else [2,3,4,5],\n",
        "        \"weight_decay\": [0.0, 0.01, 0.1],\n",
        "    }\n",
        "\n",
        "    for i in range(n_trials):\n",
        "        hparams = {\n",
        "            \"learning_rate\": random.choice(search_space[\"learning_rate\"]),\n",
        "            \"per_device_train_batch_size\": random.choice(search_space[\"per_device_train_batch_size\"]),\n",
        "            \"num_train_epochs\": random.choice(search_space[\"num_train_epochs\"]),\n",
        "            \"weight_decay\": random.choice(search_space[\"weight_decay\"]),\n",
        "        }\n",
        "        trial_name = f\"rs_trial_{i}\"\n",
        "        print(f\"=== Trial {i+1}/{n_trials}: {hparams} ===\")\n",
        "        eval_res, model_dir = run_training_trial(hparams, trial_name=trial_name)\n",
        "        acc = eval_res.get(\"eval_accuracy\", eval_res.get(\"accuracy\", None))\n",
        "        if acc is None:\n",
        "            acc = eval_res.get(\"eval_accuracy\", -999)\n",
        "        print(f\" -> Eval accuracy: {acc}\")\n",
        "        if acc is not None and acc > best_metric:\n",
        "            best_metric = acc\n",
        "            best_hparams = hparams\n",
        "            best_model_dir = model_dir\n",
        "\n",
        "    print(\"=== Random search complete ===\")\n",
        "    print(\"Best metric:\", best_metric)\n",
        "    print(\"Best hyperparameters:\", best_hparams)\n",
        "    return best_hparams, best_model_dir, best_metric\n",
        "\n",
        "# Run the random search (light by default)\n",
        "N_TRIALS = 5\n",
        "best_hparams, best_model_dir, best_metric = random_search(n_trials=N_TRIALS, short_mode=True)"
      ],
      "metadata": {
        "id": "gAbKAY5gy6Nj"
      },
      "id": "gAbKAY5gy6Nj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6) Inference: sentiment analysis + location-based congestion detection**"
      ],
      "metadata": {
        "id": "g0d5gydOybjK"
      },
      "id": "g0d5gydOybjK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "This final section of the code loads your fine-tuned sentiment model and adds an intelligent inference layer specialized for detecting potential traffic congestion from social media text — such as the tweets in your dataset data_mmda_traffic_spatial.csv.\n",
        "First, it loads the best-performing model (found during random search) into a Hugging Face sentiment-analysis pipeline, which handles tokenization, model execution, and prediction. Then it defines a set of CONGESTION_KEYWORDS — terms common in MMDA traffic reports that may indicate stalled vehicles, accidents, lane closures, or heavy traffic.\n",
        "\n",
        "**Several helper functions then support richer inference:**\n",
        "**1. extract_location(text)**\n",
        "A simple rule-based method that:\n",
        "\n",
        "- Looks for phrases following \"at\" or \"on\"\n",
        "\n",
        "- Detects ALL-CAPS text segments (common in MMDA tweets, e.g., \"EDSA\", \"C5 NB\")\n",
        "\n",
        "**2. has_congestion_keywords(text)**\n",
        "- Checks whether any congestion-related keywords are present, returning both a boolean and the keyword found.\n",
        "\n",
        "**3. infer_sentence(text)**\n",
        "\n",
        "This is the main function that takes a tweet and returns a structured result with:\n",
        "\n",
        "\n",
        "- Sentiment label (e.g., NEGATIVE / LABEL_0)\n",
        "\n",
        "\n",
        "- Confidence score\n",
        "\n",
        "\n",
        "- Extracted location (if any)\n",
        "\n",
        "\n",
        "- Congestion likelihood (\"Likely\", \"Possible\", or \"Unlikely\")\n",
        "\n",
        "\n",
        "- Reason for the decision\n",
        "\n",
        "\n",
        "It uses a rule-plus-model hybrid logic:\n",
        "Likely Congestion if:\n",
        "\n",
        "\n",
        "- The model is confident (≥ 0.8) the sentiment is negative, and\n",
        "\n",
        "\n",
        "- A congestion keyword appears in the text\n",
        "\n",
        "\n",
        "**Or the tweet is negative with location info (weaker but relevant)\n",
        "Possible Congestion if:**\n",
        "\n",
        "\n",
        "- A congestion keyword is present but sentiment confidence is low\n",
        "\n",
        "\n",
        "**Unlikely Congestion if:**\n",
        "\n",
        "\n",
        "- No strong negative signal\n",
        "\n",
        "\n",
        "- No congestion markers found\n",
        "\n",
        "\n",
        "**For each, it prints:**\n",
        "\n",
        "\n",
        "- The original text\n",
        "\n",
        "\n",
        "- Sentiment label + score from your fine-tuned model\n",
        "\n",
        "\n",
        "- Detected location (e.g., C5 Market-Market NB)\n",
        "\n",
        "\n",
        "- Congestion likelihood\n",
        "\n",
        "\n",
        "- Explanation (e.g., keyword found, high negative confidence)\n",
        "\n",
        "\n",
        "\n",
        "**In summary, this section turns your fine-tuned sentiment model into an applied traffic incident detector, leveraging:**\n",
        "\n",
        "\n",
        "- Learned sentiment patterns from your dataset\n",
        "\n",
        "\n",
        "- Traffic-specific keywords\n",
        "\n",
        "\n",
        "- Simple location extraction heuristics\n",
        "\n",
        "\n",
        "It converts raw tweets into structured, explainable traffic insights — highly relevant for analyzing incidents in Metro Manila road networks using your uploaded dataset."
      ],
      "metadata": {
        "id": "zuhAzQJovUBx"
      },
      "id": "zuhAzQJovUBx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple single-sentence inference + congestion heuristic\n",
        "from transformers import AutoModelForSequenceClassification, pipeline\n",
        "import re\n",
        "\n",
        "# Load model into pipeline (safe to re-run; will reuse if already loaded)\n",
        "try:\n",
        "    model_for_pipeline  # if pipeline already created\n",
        "except NameError:\n",
        "    model_for_pipeline = AutoModelForSequenceClassification.from_pretrained(best_model_dir)\n",
        "    sentiment_pipe = pipeline(\"sentiment-analysis\", model=model_for_pipeline, tokenizer=tokenizer,\n",
        "                              device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# congestion keywords often present in reports that cause traffic impact\n",
        "CONGESTION_KEYWORDS = [\n",
        "    \"stalled\", \"stuck\", \"breakdown\", \"mechanical problem\", \"collision\", \"accident\",\n",
        "    \"crash\", \"overturned\", \"overturn\", \"towed\", \"blocking\", \"block\", \"blocked\",\n",
        "    \"lane occupied\", \"lane closed\", \"lanes closed\", \"heavy traffic\", \"traffic jam\",\n",
        "    \"congestion\", \"pileup\", \"road closed\", \"one lane\", \"two lanes\", \"car stopped\"\n",
        "]\n",
        "\n",
        "# helper: extract a location phrase (simple heuristics)\n",
        "def extract_location(text):\n",
        "    # look for \"at <PLACE>\" or \"on <PLACE>\"\n",
        "    m = re.search(r'\\b(?:at|on)\\s+([A-Za-z0-9\\.\\-\\/\\s,&]+?)(?:[.,;]|$)', text, flags=re.I)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "    # look for ALL CAPS tokens (common in MMDA feed)\n",
        "    caps = re.findall(r'\\b[A-Z0-9]{3,}(?:\\s[A-Z0-9]{3,})*\\b', text)\n",
        "    if caps:\n",
        "        return caps[0].strip()\n",
        "    return None\n",
        "\n",
        "# helper: check presence of congestion-triggering keywords\n",
        "def has_congestion_keywords(text):\n",
        "    t = text.lower()\n",
        "    for kw in CONGESTION_KEYWORDS:\n",
        "        if kw in t:\n",
        "            return True, kw\n",
        "    return False, None\n",
        "\n",
        "# main inference function\n",
        "def infer_sentence(text, conf_threshold=0.8):\n",
        "    \"\"\"\n",
        "    Returns a small dict:\n",
        "      {\n",
        "        'text': str,\n",
        "        'sentiment_label': str,\n",
        "        'sentiment_score': float,\n",
        "        'location': str or None,\n",
        "        'congestion': 'Likely'|'Unlikely',\n",
        "        'reason': str\n",
        "      }\n",
        "    \"\"\"\n",
        "    # run HF model pipeline\n",
        "    out = sentiment_pipe(text[:1000])  # pipeline accepts a single string\n",
        "    # pipeline returns a list when passed list, but when passed single string returns a dict or list depending on version; normalize:\n",
        "    if isinstance(out, list):\n",
        "        out = out[0]\n",
        "    label_raw = str(out.get(\"label\", \"\")).upper()\n",
        "    score = float(out.get(\"score\", 0.0))\n",
        "\n",
        "    # normalize negative label detection (common formats)\n",
        "    negative_labels = {\"NEGATIVE\", \"LABEL_0\", \"0\", \"NEG\"}\n",
        "    is_negative = label_raw in negative_labels\n",
        "\n",
        "    # extract location\n",
        "    loc = extract_location(text)\n",
        "\n",
        "    # check keywords\n",
        "    kw_found, kw = has_congestion_keywords(text)\n",
        "\n",
        "    # simple heuristic:\n",
        "    # - If model says negative with high confidence AND a congestion keyword is present -> Likely congestion\n",
        "    # - If model says negative with high confidence and a location is present -> Likely (but weaker)\n",
        "    # - Otherwise -> Unlikely\n",
        "    if is_negative and score >= conf_threshold and kw_found:\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"sentiment_label\": label_raw,\n",
        "            \"sentiment_score\": score,\n",
        "            \"location\": loc,\n",
        "            \"congestion\": \"Likely congestion\",\n",
        "            \"reason\": f\"Negative ({label_raw}, score={score:.2f}) + keyword '{kw}' found.\"\n",
        "        }\n",
        "    if is_negative and score >= conf_threshold and loc:\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"sentiment_label\": label_raw,\n",
        "            \"sentiment_score\": score,\n",
        "            \"location\": loc,\n",
        "            \"congestion\": \"Likely congestion\",\n",
        "            \"reason\": f\"Negative ({label_raw}, score={score:.2f}) and location '{loc}' detected.\"\n",
        "        }\n",
        "    # fallback: if keyword present even with lower score, flag as possible\n",
        "    if kw_found:\n",
        "        return {\n",
        "            \"text\": text,\n",
        "            \"sentiment_label\": label_raw,\n",
        "            \"sentiment_score\": score,\n",
        "            \"location\": loc,\n",
        "            \"congestion\": \"Possible congestion (needs verification)\",\n",
        "            \"reason\": f\"Keyword '{kw}' found but model confidence is low ({score:.2f}).\"\n",
        "        }\n",
        "\n",
        "    # otherwise unlikely\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"sentiment_label\": label_raw,\n",
        "        \"sentiment_score\": score,\n",
        "        \"location\": loc,\n",
        "        \"congestion\": \"Unlikely congestion\",\n",
        "        \"reason\": \"No strong negative signal or congestion keywords detected.\"\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "examples = [\n",
        "    \"MMDA ALERT: Stalled SUV due to mechanical problem at C5 Market-Market NB as of 7:11 PM. 1 lane occupied.\",\n",
        "    \"Traffic flowing smoothly on EDSA near Ortigas.\",\n",
        "    \"Minor fender-bender on Shaw Boulevard, some slowdown reported.\"\n",
        "]\n",
        "\n",
        "for ex in examples:\n",
        "    r = infer_sentence(ex)\n",
        "    print(\"----\")\n",
        "    print(\"Text:\", r[\"text\"])\n",
        "    print(\"Sentiment:\", r[\"sentiment_label\"], f\"({r['sentiment_score']:.2f})\")\n",
        "    print(\"Location:\", r[\"location\"])\n",
        "    print(\"Congestion:\", r[\"congestion\"])\n",
        "    print(\"Reason:\", r[\"reason\"])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "IcYktIlxy_Oj"
      },
      "id": "IcYktIlxy_Oj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}